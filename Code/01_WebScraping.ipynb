{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping for New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from newsplease import NewsPlease\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cal = []\n",
    "url_year = []\n",
    "url_list = []\n",
    "url_list1=[]\n",
    "url_each=[]\n",
    "\n",
    "#Creating all the Dates from Calendar package for URL\n",
    "c = calendar.TextCalendar()\n",
    "for rr in range(1,13):\n",
    "    for i in c.itermonthdates(2018,rr):\n",
    "        j=str(i)\n",
    "        j=j.replace('-','/')\n",
    "        cal.append(j)\n",
    "sortedCal = list(dict.fromkeys(cal))\n",
    "\n",
    "#Creating the URL for the whole year of 2018\n",
    "start_Url = 'https://www.nytimes.com/issue/todayspaper/'\n",
    "end_Url = '/todays-new-york-times'\n",
    "\n",
    "for dates in sortedCal:\n",
    "    link = start_Url + dates + end_Url\n",
    "    url_year.append(link)\n",
    "    \n",
    "for urls in url_year:\n",
    "    page = requests.get(urls)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    for url in soup.find_all('a'):\n",
    "        a = url.get('href')\n",
    "        if a== None:\n",
    "            a\n",
    "        else:\n",
    "            if a.startswith('/'):\n",
    "                url_list.append(a)\n",
    "    for u in url_list:\n",
    "        if u != '/':\n",
    "            url_list1.append(u)\n",
    "    start_url = 'https://www.nytimes.com'\n",
    "    for ab in url_list1:\n",
    "        c = start_url+ab\n",
    "        url_each.append(c)\n",
    "url_each1 = list(dict.fromkeys(url_each))\n",
    "# print(len(url_each))\n",
    "# print(len(url_each1))\n",
    "\n",
    "#Web Scraping of all the articles\n",
    "id_csv=[]\n",
    "title_csv = []\n",
    "publication_csv = []\n",
    "author_csv = []\n",
    "date_csv = []\n",
    "url_csv = []\n",
    "content_csv=[]\n",
    "company_csv =[]\n",
    "category_csv=[]\n",
    "year_csv=[]\n",
    "month_csv=[]\n",
    "\n",
    "count=0\n",
    "for artic in url_each1:\n",
    "    article = NewsPlease.from_url(artic)\n",
    "    if (article.text) == None:\n",
    "        print('None')\n",
    "    elif len(article.text) >32767:\n",
    "        print('Cant add in excel')\n",
    "    else:\n",
    "        title = article.title\n",
    "        title_csv.append(title)\n",
    "\n",
    "        author = article.authors\n",
    "        author_csv.append(author)\n",
    "\n",
    "        url = article.url\n",
    "        url_csv.append(url)\n",
    "\n",
    "        content = article.text\n",
    "        content_csv.append(content)\n",
    "        \n",
    "        if article.date_publish ==None:\n",
    "            publication_csv.append('None')\n",
    "            month_csv.append('None')\n",
    "            year_csv.append('None')\n",
    "            category_csv.append('None')\n",
    "        else:\n",
    "            dates = article.date_publish.strftime(\"%x\")\n",
    "            publication_csv.append(dates)\n",
    "\n",
    "            month = article.date_publish.month\n",
    "            month_csv.append(month)\n",
    "\n",
    "            year = article.date_publish.year\n",
    "            year_csv.append(year)\n",
    "\n",
    "            c=url.find('interactive')\n",
    "            a= url.split('.com/')\n",
    "            b=a[1]\n",
    "            if c>0:\n",
    "                x = b.split(\"/\", 5)\n",
    "                category=x[4]\n",
    "            else:\n",
    "                x = b.split(\"/\", 4)\n",
    "                category=x[3]\n",
    "            category_csv.append(category)\n",
    "            \n",
    "        company_csv.append('NY Times')\n",
    "        count=count+1\n",
    "#        print(count,' Done')\n",
    "    \n",
    "id_len=len(title_csv)\n",
    "for ids in range(1,id_len+1):\n",
    "    id_csv.append(ids)\n",
    "    \n",
    "df = pd.DataFrame(data={'ID':id_csv,'Title': title_csv,'Publication':company_csv,'Author': author_csv,\n",
    "                            'Published': publication_csv,'Year':year_csv,'Month':month_csv,'Category':category_csv,'URL': url_csv, 'Content': content_csv})\n",
    "df.to_csv(os.getcwd()+'/Data_NY.csv')\n",
    "print('CSV Created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping for Tampa Bay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from newsplease import NewsPlease\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cal = []\n",
    "url_year = []\n",
    "url_list = []\n",
    "url_list1=[]\n",
    "url_list2=[]\n",
    "url_each=[]\n",
    "\n",
    "id_csv=[]\n",
    "title_csv = []\n",
    "publication_csv = []\n",
    "author_csv = []\n",
    "date_csv = []\n",
    "url_csv = []\n",
    "content_csv=[]\n",
    "company_csv =[]\n",
    "category_csv=[]\n",
    "year_csv=[]\n",
    "month_csv=[]\n",
    "\n",
    "#Creating all the Dates from Calendar package for URL\n",
    "c = calendar.TextCalendar()\n",
    "for rr in range(1,13):\n",
    "    for i in c.itermonthdates(2018,rr):\n",
    "        j=str(i)\n",
    "        j=j.replace('-','')\n",
    "        cal.append(j)\n",
    "sortedCal = list(dict.fromkeys(cal))\n",
    "\n",
    "#Creating the URL for the whole year of 2018\n",
    "start_Url = 'https://www.tampabay.com/todays-paper/?date='\n",
    "\n",
    "for dates in sortedCal:\n",
    "    link = start_Url + dates\n",
    "    url_year.append(link)\n",
    "    \n",
    "for urls in url_year[240:250]:\n",
    "    page = requests.get(urls)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    for url in soup.find_all('a'):\n",
    "        a = url.get('href')\n",
    "        if a== None:\n",
    "            a\n",
    "        else:\n",
    "            if a.startswith('/'):\n",
    "                if len(a) > 32:\n",
    "                    url_list.append(a)\n",
    "    start_url = 'https://www.tampabay.com'\n",
    "    for ab in url_list:\n",
    "        c = start_url+ab\n",
    "        url_each.append(c)\n",
    "url_each1 = list(dict.fromkeys(url_each))\n",
    "# print(len(url_each))\n",
    "# print(len(url_each1))\n",
    "\n",
    "#Web Scraping of all the articles\n",
    "count=0\n",
    "for artic in url_each1:\n",
    "    article = NewsPlease.from_url(artic)\n",
    "    if len(article.text) >32767:\n",
    "        print('Cant add in excel')\n",
    "    else:\n",
    "        title = article.title\n",
    "        title_csv.append(title)\n",
    "\n",
    "        author = article.authors\n",
    "        author_csv.append(author)\n",
    "\n",
    "        url = article.url\n",
    "        url_csv.append(url)\n",
    "\n",
    "        content = article.text\n",
    "        content_csv.append(content)\n",
    "\n",
    "        dates = article.date_publish.strftime(\"%x\")\n",
    "        publication_csv.append(dates)\n",
    "\n",
    "        month = article.date_publish.month\n",
    "        month_csv.append(month)\n",
    "\n",
    "        year = article.date_publish.year\n",
    "        year_csv.append(year)\n",
    "\n",
    "        company_csv.append('TampaBay')\n",
    "\n",
    "        page = requests.get(artic)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        a= url.split('.com/')\n",
    "        b=a[1]\n",
    "        x = b.split(\"/\", 1)\n",
    "        category=x[0]\n",
    "        category\n",
    "        category_csv.append(category)\n",
    "        count=count+1\n",
    "#        print(count,' Done')\n",
    "    \n",
    "id_len=len(title_csv)\n",
    "for ids in range(1,id_len+1):\n",
    "    id_csv.append(ids)\n",
    "    \n",
    "df = pd.DataFrame(data={'ID':id_csv,'Title': title_csv,'Publication':company_csv,'Author': author_csv,\n",
    "                            'Published': publication_csv,'Year':year_csv,'Month':month_csv,'Category':category_csv,'URL': url_csv, 'Content': content_csv})\n",
    "df.to_csv(os.getcwd()+'/Data_Tampa_240.csv')\n",
    "print('CSV Created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping for Wall Street Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from newsplease import NewsPlease\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cal = []\n",
    "url_year = []\n",
    "url_list = []\n",
    "url_each=[]\n",
    "\n",
    "id_csv=[]\n",
    "title_csv = []\n",
    "publication_csv = []\n",
    "author_csv = []\n",
    "date_csv = []\n",
    "url_csv = []\n",
    "content_csv=[]\n",
    "company_csv =[]\n",
    "category_csv=[]\n",
    "year_csv=[]\n",
    "month_csv=[]\n",
    "\n",
    "#Creating all the Dates from Calendar package for URL\n",
    "c = calendar.TextCalendar()\n",
    "for rr in range(1,13):\n",
    "    for i in c.itermonthdates(2018,rr):\n",
    "        j=str(i)\n",
    "        j=j.replace('-','')\n",
    "        cal.append(j)\n",
    "sortedCal = list(dict.fromkeys(cal))\n",
    "\n",
    "#Creating the URL for the whole year of 2018\n",
    "start_Url = 'https://www.wsj.com/news/archive/'\n",
    "\n",
    "for dates in sortedCal:\n",
    "    link = start_Url + dates\n",
    "    url_year.append(link)\n",
    "    \n",
    "for urls in url_year[182:]:\n",
    "    page = requests.get(urls)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    for url in soup.find_all('a'):\n",
    "        a = url.get('href')\n",
    "        if a== None:\n",
    "            a\n",
    "        else:\n",
    "            if a.startswith('https://www.wsj.com/articles'):\n",
    "                url_list.append(a)\n",
    "url_each = list(dict.fromkeys(url_list))\n",
    "# print(len(url_list))\n",
    "# print(len(url_each))\n",
    "\n",
    "#Web Scraping of all the articles\n",
    "count = 0\n",
    "for artic in url_each:\n",
    "    article = NewsPlease.from_url(artic)\n",
    "    if (article.text) == None:\n",
    "        print('None')\n",
    "    elif len(article.text) >32767:\n",
    "        print('Cant add in excel')\n",
    "    else:\n",
    "        title = article.title\n",
    "        title_csv.append(title)\n",
    "\n",
    "        author = article.authors\n",
    "        author_csv.append(author)\n",
    "\n",
    "        url = article.url\n",
    "        url_csv.append(url)\n",
    "\n",
    "        content = article.text\n",
    "        content_csv.append(content)\n",
    "\n",
    "        dates = article.date_publish.strftime(\"%x\")\n",
    "        publication_csv.append(dates)\n",
    "\n",
    "        month = article.date_publish.month\n",
    "        month_csv.append(month)\n",
    "\n",
    "        year = article.date_publish.year\n",
    "        year_csv.append(year)\n",
    "\n",
    "        company_csv.append('WSJ')\n",
    "\n",
    "        page = requests.get(artic)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        if soup.find_all('span',{'itemprop':'name'}) == []:\n",
    "            category = 'None'\n",
    "        else:\n",
    "            for category in soup.find_all('span',{'itemprop':'name'})[0]:\n",
    "                category\n",
    "        category_csv.append(category)\n",
    "        count=count+1\n",
    "#        print(count, ' Done')\n",
    "    \n",
    "id_len=len(title_csv)\n",
    "for ids in range(1,id_len+1):\n",
    "    id_csv.append(ids)\n",
    "    \n",
    "df = pd.DataFrame(data={'ID':id_csv,'Title': title_csv,'Publication':company_csv,'Author': author_csv,\n",
    "                            'Published': publication_csv,'Year':year_csv,'Month':month_csv,'Category':category_csv,'URL': url_csv, 'Content': content_csv})\n",
    "df.to_csv(os.getcwd()+'/Data_WSJ2.csv')\n",
    "print('CSV Created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebScraping for FAKE News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webhoseio\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "\n",
    "content = []\n",
    "title = []\n",
    "publication = []\n",
    "author = []\n",
    "date = []\n",
    "year = []\n",
    "month = []\n",
    "url = []\n",
    "id_nbr = []\n",
    "category = []\n",
    "\n",
    "webhoseio.config(token=\"\")\n",
    "query_params = {\n",
    "    \"q\": \"language:english site_type:news thread.country:US site_category:media\",\n",
    "    \"ts\": \"1552763507853\",\n",
    "    \"sort\": \"crawled\"\n",
    "}\n",
    "output = webhoseio.query(\"filterWebContent\", query_params)\n",
    "print(output['totalResults'])\n",
    "print(len(output['posts']))\n",
    "\n",
    "count = 1\n",
    "for y in range(output['totalResults']):\n",
    "    for x in range(len(output['posts'])):\n",
    "        content.append(output['posts'][x]['text']) # Print the text of the first post\n",
    "        publication.append(output['posts'][x]['thread']['site'])\n",
    "        \n",
    "        dateformat = parse(output['posts'][x]['published'])\n",
    "        \n",
    "        article_date = dateformat.date()\n",
    "        date.append(article_date) # Print the text of the first post publication date\n",
    "        \n",
    "        article_year = dateformat.year\n",
    "        year.append(article_year)\n",
    "       \n",
    "        article_month = dateformat.month\n",
    "        month.append(article_month)\n",
    "        \n",
    "        author.append(output['posts'][x]['author'])\n",
    "        title.append(output['posts'][x]['title'])\n",
    "\n",
    "        url.append(output['posts'][x]['thread']['url'])\n",
    "        id_nbr.append(count)\n",
    "        \n",
    "        list_size = len(output['posts'][x]['thread']['site_categories'])\n",
    "        if list_size > 1:\n",
    "            category.append(output['posts'][x]['thread']['site_categories'][-1])\n",
    "        else:\n",
    "            category.append(output['posts'][x]['thread']['site_categories'][0])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Get the next batch of posts\n",
    "    output = webhoseio.get_next()\n",
    "    if y==850:\n",
    "        break\n",
    "\n",
    "fake_news_dict = {'ID':id_nbr, 'Title': title,'Publication': publication, 'Author' : author, 'Published':date,\n",
    "                  'Year':year,'Month':month,'Category':category,'URL':url,'Content':content}\n",
    "\n",
    "df = DataFrame(fake_news_dict, columns= ['ID','Title', 'Publication','Author','Published','Year','Month','Category',\n",
    "                                         'URL','Content'])\n",
    "# export_csv = df.to_csv (r'C:\\Users\\Ron\\Desktop\\export_dataframe.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n",
    "path = os.getcwd()\n",
    "df.to_csv(os.path.join(path,r'fakeNews.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
